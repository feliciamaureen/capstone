{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lyric files\n",
    "eurovision = open(\"data/intersection.txt\", \"r\")\n",
    "c60s = open(\"data/1960s.txt\", \"r\")\n",
    "c70s = open(\"data/1970s.txt\", \"r\")\n",
    "c80s = open(\"data/1980s.txt\", \"r\")\n",
    "c90s = open(\"data/1990s.txt\", \"r\")\n",
    "c00s = open(\"data/2000s.txt\", \"r\")\n",
    "c10s = open(\"data/2010s.txt\", \"r\")\n",
    "\n",
    "eurovisionLyrics = eurovision.read()\n",
    "lyrics60s = c60s.read()\n",
    "lyrics70s = c70s.read()\n",
    "lyrics80s = c80s.read()\n",
    "lyrics90s = c90s.read()\n",
    "lyrics00s = c00s.read()\n",
    "lyrics10s = c10s.read()\n",
    "\n",
    "eurovision.close()\n",
    "c60s.close()\n",
    "c70s.close()\n",
    "c80s.close()\n",
    "c90s.close()\n",
    "c00s.close()\n",
    "c10s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make lowercase\n",
    "def convLower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "eurovisionLyrics = convLower(eurovisionLyrics)\n",
    "lyrics60s = convLower(lyrics60s)\n",
    "lyrics70s = convLower(lyrics70s)\n",
    "lyrics80s = convLower(lyrics80s)\n",
    "lyrics90s = convLower(lyrics90s)\n",
    "lyrics00s = convLower(lyrics00s)\n",
    "lyrics10s = convLower(lyrics10s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation - this doesnt remove apostrophes\n",
    "def removePunctuation(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "eurovisionLyrics = removePunctuation(eurovisionLyrics)\n",
    "lyrics60s = removePunctuation(lyrics60s)\n",
    "lyrics70s = removePunctuation(lyrics70s)\n",
    "lyrics80s = removePunctuation(lyrics80s)\n",
    "lyrics90s = removePunctuation(lyrics90s)\n",
    "lyrics00s = removePunctuation(lyrics00s)\n",
    "lyrics10s = removePunctuation(lyrics10s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokenize\n",
    "wordsEurovision = word_tokenize(eurovisionLyrics)\n",
    "words60s = word_tokenize(lyrics60s)\n",
    "words70s = word_tokenize(lyrics70s)\n",
    "words80s = word_tokenize(lyrics80s)\n",
    "words90s = word_tokenize(lyrics90s)\n",
    "words00s = word_tokenize(lyrics00s)\n",
    "words10s = word_tokenize(lyrics10s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenize\n",
    "senEurovision = sent_tokenize(eurovisionLyrics)\n",
    "sen60s = sent_tokenize(lyrics60s)\n",
    "sen70s = sent_tokenize(lyrics70s)\n",
    "sen80s = sent_tokenize(lyrics80s)\n",
    "sen90s = sent_tokenize(lyrics90s)\n",
    "sen00s = sent_tokenize(lyrics00s)\n",
    "sen10s = sent_tokenize(lyrics10s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test frequency dist\n",
    "words10sFD = FreqDist(words10s)\n",
    "words10sFreqDist = pd.DataFrame(list(words10sFD.items()), columns = [\"Word\",\"Frequency\"]) \n",
    "\n",
    "#words10sFD.most_common(50)\n",
    "\n",
    "wordsEurovisionFD = FreqDist(wordsEurovision)\n",
    "wordsEurovisionFreqDist = pd.DataFrame(list(wordsEurovisionFD.items()), columns = [\"Word\",\"Frequency\"]) \n",
    "\n",
    "#wordsEurovisionFD.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('thunder', 'thunder'), 49),\n",
       " (('dohdohdoh', 'dohdoh'), 24),\n",
       " (('dohdohdoh', 'dohdohdoh'), 23)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams 2010s\n",
    "bigrams10s = nltk.bigrams(words10s)\n",
    "filtered_bigrams = [ (w1, w2) for w1, w2 in bigrams10s if len(w1) >=5 and len(w2) >= 5 ]\n",
    "eng_bifreq = nltk.FreqDist(filtered_bigrams)\n",
    "eng_bifreq.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('think', 'youre'), 40), (('youre', 'ready'), 38), (('music', 'music'), 27)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams 2000s\n",
    "bigrams00s = nltk.bigrams(words00s)\n",
    "filtered_bigrams = [ (w1, w2) for w1, w2 in bigrams00s if len(w1) >=5 and len(w2) >= 5 ]\n",
    "eng_bifreq = nltk.FreqDist(filtered_bigrams)\n",
    "eng_bifreq.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('mercy', 'mercy'), 28),\n",
       " (('strong', 'enough'), 23),\n",
       " (('youre', 'still'), 21)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams 1990s\n",
    "bigrams90s = nltk.bigrams(words90s)\n",
    "filtered_bigrams = [ (w1, w2) for w1, w2 in bigrams90s if len(w1) >=5 and len(w2) >= 5 ]\n",
    "eng_bifreq = nltk.FreqDist(filtered_bigrams)\n",
    "eng_bifreq.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('louie', 'louie'), 22),\n",
       " (('karma', 'karma'), 16),\n",
       " (('invisible', 'touch'), 16)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams 1980s\n",
    "bigrams80s = nltk.bigrams(words80s)\n",
    "filtered_bigrams = [ (w1, w2) for w1, w2 in bigrams80s if len(w1) >=5 and len(w2) >= 5 ]\n",
    "eng_bifreq = nltk.FreqDist(filtered_bigrams)\n",
    "eng_bifreq.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('stayin', 'alive'), 25),\n",
       " (('doodoo', 'doodoodoo'), 20),\n",
       " (('really', 'hurts'), 18)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams 1970s\n",
    "bigrams70s = nltk.bigrams(words70s)\n",
    "filtered_bigrams = [ (w1, w2) for w1, w2 in bigrams70s if len(w1) >=5 and len(w2) >= 5 ]\n",
    "eng_bifreq = nltk.FreqDist(filtered_bigrams)\n",
    "eng_bifreq.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('first', 'communion'), 41),\n",
       " (('talking', 'about'), 28),\n",
       " (('bouncy', 'bouncy'), 26)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams 1960s\n",
    "bigrams60s = nltk.bigrams(words60s)\n",
    "filtered_bigrams = [ (w1, w2) for w1, w2 in bigrams60s if len(w1) >=5 and len(w2) >= 5 ]\n",
    "eng_bifreq = nltk.FreqDist(filtered_bigrams)\n",
    "eng_bifreq.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('these', 'words'), 10), (('every', 'night'), 10), (('mamma', 'mamma'), 10)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams Eurovision\n",
    "bigramsEurovision = nltk.bigrams(wordsEurovision)\n",
    "filtered_bigrams = [ (w1, w2) for w1, w2 in bigramsEurovision if len(w1) >=5 and len(w2) >= 5 ]\n",
    "eng_bifreq = nltk.FreqDist(filtered_bigrams)\n",
    "eng_bifreq.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
